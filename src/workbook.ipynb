{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectural_principles import ArchitecturalConstraints\n",
    "from train import ExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'env': {\n",
    "        'grid_size': (10, 10),\n",
    "        'max_steps': 500,\n",
    "        'required_rooms': ArchitecturalConstraints.default_rooms()\n",
    "    },\n",
    "    'algorithms': {\n",
    "        'value_iteration': {\n",
    "            'gamma': .95,\n",
    "            'theta': 0.001\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exm = ExperimentManager(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exm.run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from torch.multiprocessing import Pool\n",
    "\n",
    "\n",
    "def generate_room_dimensions_for_room(room_ranges_items: tuple):\n",
    "        \"\"\"\n",
    "        Generate the dimensions (width, height) combinations for a single room.\n",
    "        This is a helper function that can be run in parallel.\n",
    "        \"\"\"\n",
    "        room, (w_range, h_range) = room_ranges_items\n",
    "        widths = np.arange(w_range[0], w_range[1] + 1)\n",
    "        heights = np.arange(h_range[0], h_range[1] + 1)\n",
    "        # Create a mesh grid of all width and height combinations\n",
    "        w_grid, h_grid = np.meshgrid(widths, heights)\n",
    "        # Stack the grids to get a list of (w, h) tuples\n",
    "        room_dimensions = np.vstack([w_grid.ravel(), h_grid.ravel()]).T\n",
    "        return room, room_dimensions\n",
    "\n",
    "\n",
    "# Check overlap using batched parallel computation\n",
    "def check_overlaps(positions_batch):\n",
    "    batch_size = positions_batch.size(0)\n",
    "    grid = torch.zeros((batch_size, G_width, G_height), dtype=torch.int32, device='cuda')  # Use int32 for counting\n",
    "\n",
    "    # Iterate over the batch and unpack each room's coordinates (x, y, w, h)\n",
    "    for i in range(batch_size):\n",
    "        room = positions_batch[i]  # Now room is a tensor of shape [4] (x, y, w, h)\n",
    "        \n",
    "        # Ensure that room is a 1D tensor (x, y, w, h)\n",
    "        if room.dim() != 1 or room.size(0) != 4:\n",
    "            print(f\"Unexpected room shape: {room.shape}\")\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = room[0].item(), room[1].item(), room[2].item(), room[3].item()  # Get scalar values\n",
    "\n",
    "        # Fill grid for each configuration in batch\n",
    "        for bx in range(x, x + w):\n",
    "            for by in range(y, y + h):\n",
    "                grid[i, bx, by] += 1  # Increment cells for each room's area\n",
    "\n",
    "    # Valid if all grid cells are <= 1\n",
    "    return (grid <= 1).all(dim=(1, 2))  # Check if no cell is occupied by more than 1 room\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_room_configs_parallel(room_ranges: dict, grid_size: tuple):\n",
    "    G_width, G_height = grid_size\n",
    "\n",
    "    # Create ranges for each room type\n",
    "    room_types = list(room_ranges.keys())\n",
    "\n",
    "    def generate_room_dimensions_parallel(room_ranges):\n",
    "        \"\"\"\n",
    "        Generate room dimensions for all rooms in parallel.\n",
    "        \"\"\"\n",
    "        dimensions = {}        \n",
    "        # Use ProcessPoolExecutor to parallelize the generation of dimensions\n",
    "        executor = concurrent.futures.ThreadPoolExecutor()\n",
    "        for result in executor.map(generate_room_dimensions_for_room, room_ranges.items()):\n",
    "            room, room_dimensions = result\n",
    "            dimensions[room] = room_dimensions\n",
    "        return dimensions\n",
    "    \n",
    "    dimensions = generate_room_dimensions_parallel(room_ranges)\n",
    "    # Generate all possible positions for a room given its dimensions\n",
    "    def generate_positions(dimensions):\n",
    "        positions = []\n",
    "        for w, h in dimensions:\n",
    "            for x in range(G_width - w + 1):\n",
    "                for y in range(G_height - h + 1):\n",
    "                    positions.append((x, y, w, h))\n",
    "        return positions\n",
    "\n",
    "    # Generate position tensors for all rooms\n",
    "    all_positions = [torch.tensor(generate_positions(dimensions[room]), dtype=torch.int32, device='cuda') for room in room_types]\n",
    "    all_positions_flattend = [pos.flatten() for pos in all_positions]\n",
    "    product = torch.meshgrid(*all_positions_flattend, indexing='ij')\n",
    "    all_batches = torch.stack(product, dim=-1).reshape(-1, len(all_positions_flattend))\n",
    "    print(\"batched\")\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(check_overlaps, all_batches)\n",
    "\n",
    "    # Combine all room positions in parallel, iterate over all possible combinations of room placements\n",
    "    valid_combinations = []\n",
    "    \n",
    "    # Generate all possible combinations of positions for each room\n",
    "    from itertools import product\n",
    "    for batch in product(*all_positions):\n",
    "        batch_tensor = torch.stack(batch, dim=0).to('cuda')  # Stack into a single tensor of shape (num_rooms, num_positions, 4)\n",
    "        # Check for overlap, ensure the result is True for all rooms in the batch\n",
    "        if check_overlaps(batch_tensor).all():  # .all() ensures the batch is valid\n",
    "            valid_combinations.append(batch_tensor.cpu().tolist())\n",
    "        \n",
    "    return valid_combinations\n",
    "\n",
    "\n",
    "# Example usage\n",
    "room_ranges = {\n",
    "    \"R1\": ((2, 4), (2, 4)),\n",
    "    \"R2\": ((1, 3), (1, 3)),\n",
    "#     \"R3\": ((3, 5), (3, 5)),\n",
    "#     \"R4\": ((2, 3), (2, 3)),\n",
    "#     \"R5\": ((1, 2), (1, 2)),\n",
    "#     \"R6\": ((2, 5), (1, 4))  # Added a 6th room type\n",
    "}\n",
    "grid_size = (10, 10)\n",
    "states = generate_room_configs_parallel(room_ranges, grid_size)\n",
    "print(f\"Total valid configurations: {len(states)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
